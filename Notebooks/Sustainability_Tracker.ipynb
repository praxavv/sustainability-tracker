{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "> Project Title: **Sustainability-Tracker**\n",
        "\n",
        "*Interactive Weather Forecasting with BigQuery AI*\n",
        "\n",
        "**Problem Statement:**\n",
        "Accurate short-term temperature prediction is essential for agriculture, energy planning, and disaster preparedness. Traditional weather dashboards either overwhelm users with raw data or hide the logic behind black-box forecasts. This project tackles the challenge of making weather prediction both accurate and transparent by combining historical GSOD data, machine learning models, and interactive visualizations into a single, user-friendly platform.\n",
        "\n",
        "Note: This is a sample notebook built for demonstration. The heavy lifting (data engineering, model training, and full-scale pipelines) was done in the original notebook‚Äîhere we focus on a lighter, interactive version to showcase the workflow."
      ],
      "metadata": {
        "id": "E-k3K8Mi8ksk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Loading Raw Data ---\n",
        "import pandas as pd\n",
        "import json\n",
        "import requests\n",
        "\n",
        "# Fetching sample data\n",
        "csv_url = \"https://github.com/praxavv/sustainability-tracker/raw/refs/heads/main/Data/sample_gsod_last10.csv\"\n",
        "json_url = \"https://github.com/praxavv/sustainability-tracker/raw/refs/heads/main/Data/gsod_metadata.json\"\n",
        "\n",
        "# Load CSV\n",
        "df = pd.read_csv(csv_url)\n",
        "\n",
        "# Load JSON\n",
        "response = requests.get(json_url)\n",
        "meta = response.json()\n",
        "\n",
        "# Display metadata\n",
        "print(\"\\nDataset Metadata:\")\n",
        "print(f\"Total Rows: {meta['total_rows']}\")\n",
        "print(f\"Shape: {tuple(meta['shape'])}\")\n",
        "print(f\"Column Count: {meta['column_count']}\")\n",
        "print(\"Columns:\", meta['columns'])\n",
        "\n",
        "# Display last 10 rows\n",
        "print(\"\\nLast 10 rows:\")\n",
        "display(df.T)"
      ],
      "metadata": {
        "id": "rxpjkGXZgRuV",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Exploration:\n",
        "We leveraged **NOAA‚Äôs GSOD** dataset (2018‚Äì2024) hosted on BigQuery as the foundation for our analysis. The exploration phase began by examining dataset scale‚Äîcounting the total rows and inspecting the last ten records‚Äîto confirm consistency and completeness. We then extracted metadata, including overall shape, column count, and variable names, to build a clear understanding of the dataset‚Äôs structure before moving into preprocessing and modeling."
      ],
      "metadata": {
        "id": "5KrkMi7vIDM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cleaned Data ---\n",
        "import pandas as pd\n",
        "import json\n",
        "import requests\n",
        "\n",
        "# Cleaned Data\n",
        "csv_url = \"https://github.com/praxavv/sustainability-tracker/raw/refs/heads/main/Data/cleaned-data.csv\"\n",
        "json_url = \"https://github.com/praxavv/sustainability-tracker/raw/refs/heads/main/Data/cleaned-metadata.json\"\n",
        "\n",
        "# Load CSV\n",
        "df = pd.read_csv(csv_url)\n",
        "\n",
        "# Load JSON\n",
        "response = requests.get(json_url)\n",
        "meta = response.json()\n",
        "\n",
        "# Display metadata\n",
        "print(\"Cleaned Data:\\n\")\n",
        "print(f\"Total Rows: {meta['total_rows']}\")\n",
        "print(f\"Shape: {tuple(meta['shape'])}\")\n",
        "print(f\"Column Count: {meta['column_count']}\")\n",
        "print(\"Columns:\", meta['columns'])\n",
        "\n",
        "# Display last 10 rows\n",
        "print(\"\\nLast 10 rows:\")\n",
        "display(df.T)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Mv5YH1V08OV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Preprocessed Daily Dataset\n",
        "The gsod_daily_aggregated table holds cleaned and standardized daily weather observations, ready for analysis and modeling.\n",
        "\n",
        "Preprocessing highlights:\n",
        "\n",
        "a. Converted units for consistency (¬∞F ‚Üí ¬∞C, mph ‚Üí m/s)\n",
        "\n",
        "b. Removed invalid or missing values\n",
        "\n",
        "c. Enriched records by joining station metadata (name, country, latitude, longitude)\n",
        "\n",
        "To validate these transformations, we inspected the last 10 rows and confirmed the overall dataset shape, ensuring the preprocessing pipeline produced a reliable foundation for modeling.\n",
        "\n",
        "> Note on Precision:\n",
        "\n",
        "\n",
        "Predictions are reported with high decimal precision to preserve accuracy. While this may produce outputs with many decimal places, the detail ensures results remain statistically robust."
      ],
      "metadata": {
        "id": "WHtZqKgXIVe-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Data for visualization\n",
        "url = \"https://github.com/praxavv/sustainability-tracker/raw/refs/heads/main/Data/yearly_metadata.json\"\n",
        "df_yearly = pd.read_json(url)\n",
        "\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "plt.rcParams.update({'figure.figsize': (15,10), 'font.size': 12})\n",
        "\n",
        "fig, axs = plt.subplots(2,2)\n",
        "\n",
        "# 1. Temperature trends with min-max band\n",
        "axs[0,0].plot(df_yearly.year, df_yearly.avg_temp, color='tomato', label='Mean Temp')\n",
        "axs[0,0].fill_between(df_yearly.year, df_yearly.min_temp, df_yearly.max_temp,\n",
        "                      color='tomato', alpha=0.2, label='Min-Max')\n",
        "axs[0,0].set_title(\"Annual Temperature Trends (¬∞C)\")\n",
        "axs[0,0].set_xlabel(\"Year\"); axs[0,0].set_ylabel(\"Temperature (¬∞C)\")\n",
        "axs[0,0].legend()\n",
        "\n",
        "# 2. Precipitation trends\n",
        "axs[0,1].bar(df_yearly.year, df_yearly.total_precip, color='royalblue', alpha=0.7)\n",
        "axs[0,1].plot(df_yearly.year, df_yearly.avg_precip, color='navy', marker='o', label='Avg Precip')\n",
        "axs[0,1].set_title(\"Annual Precipitation (mm)\")\n",
        "axs[0,1].set_xlabel(\"Year\"); axs[0,1].set_ylabel(\"Precipitation (mm)\")\n",
        "axs[0,1].legend()\n",
        "\n",
        "# 3. Wind trends\n",
        "axs[1,0].plot(df_yearly.year, df_yearly.avg_wind, color='seagreen', marker='s')\n",
        "axs[1,0].set_title(\"Average Annual Wind Speed (m/s)\")\n",
        "axs[1,0].set_xlabel(\"Year\"); axs[1,0].set_ylabel(\"Wind Speed (m/s)\")\n",
        "\n",
        "# 4. Correlation Heatmap\n",
        "corr = df_yearly[['avg_temp','avg_precip','avg_wind']].corr()\n",
        "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap='coolwarm', ax=axs[1,1], square=True)\n",
        "axs[1,1].set_title(\"Climate Metrics Correlation\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "TudNvF6U9HQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Yearly Climate Statistics**\n",
        "\n",
        "Daily data was aggregated to compute annual statistics across temperature, precipitation, and wind.\n",
        "\n",
        "> Visualizations include:\n",
        "\n",
        "**Temperature**: annual mean with min‚Äìmax range\n",
        "\n",
        "**Precipitation**: yearly totals and average trends\n",
        "\n",
        "**Wind**: average annual speed\n",
        "\n",
        "**Correlation**: relationships between key climate metrics\n",
        "\n",
        "Together, these plots offer a high-level view of long-term climate patterns.\n",
        "\n",
        "*Note: The visualizations are based on NOAA datasets and are intended to provide insights into climate trends. Actual local conditions may differ and should be interpreted with context.*"
      ],
      "metadata": {
        "id": "0l3gQ5wNI8OW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np, pandas as pd, plotly.graph_objects as go\n",
        "\n",
        "# config / data\n",
        "R = 1\n",
        "url = \"https://github.com/praxavv/sustainability-tracker/raw/refs/heads/main/Data/Globe.parquet\"\n",
        "df = pd.read_parquet(url)\n",
        "\n",
        "# sphere coords\n",
        "lat = np.radians(df['latitude'].values); lon = np.radians(df['longitude'].values)\n",
        "x = R * np.cos(lat) * np.cos(lon)\n",
        "y = R * np.cos(lat) * np.sin(lon)\n",
        "z = R * np.sin(lat)\n",
        "\n",
        "# hover text (vectorized via apply)\n",
        "hover_text = df.apply(\n",
        "    lambda r: (\n",
        "        f\"<b>{r.station_name} ({r.country_code})</b><br><br>\"\n",
        "        f\"Avg Temp: {r.temp_c} ¬∞C<br>\"\n",
        "        f\"Avg Dewp: {r.dewp_c} ¬∞C<br>\"\n",
        "        f\"Avg Wind: {r.wind_speed_m_s} m/s<br>\"\n",
        "        f\"Avg Visibility: {r.visibility_km} km<br>\"\n",
        "        f\"Avg Precipitation: {r.precipitation_mm} mm<br>\"\n",
        "        f\"Avg Pressure: {r.pressure_hpa} hPa\"\n",
        "    ),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# globe surface grid\n",
        "phi, theta = np.mgrid[0:np.pi:50j, 0:2*np.pi:100j]\n",
        "xs = R * np.sin(phi) * np.cos(theta)\n",
        "ys = R * np.sin(phi) * np.sin(theta)\n",
        "zs = R * np.cos(phi)\n",
        "\n",
        "BLUE, WHITE, NEON_ORANGE = \"#0066FF\", \"#FFFFFF\", \"#FF6A00\"\n",
        "\n",
        "fig = go.Figure(data=[\n",
        "    go.Surface(x=xs, y=ys, z=zs, colorscale=[[0,\"black\"],[1,\"black\"]],\n",
        "               showscale=False, hoverinfo=\"skip\", opacity=1.0),\n",
        "    go.Scatter3d(\n",
        "        x=x, y=y, z=z, mode=\"markers\",\n",
        "        marker=dict(\n",
        "            size=2, sizemode=\"diameter\", color=df[\"temp_c\"].values,\n",
        "            colorscale=[[0.0, BLUE], [0.44, WHITE], [1.0, NEON_ORANGE]],\n",
        "            cmin=-40, cmax=50,\n",
        "            colorbar=dict(title=\"Avg Temp (¬∞C)\", thickness=15, len=0.5, x=1.05, y=0.8),\n",
        "            opacity=1.0, line=dict(width=0.2, color=\"black\")\n",
        "        ),\n",
        "        hoverinfo=\"skip\", showlegend=False\n",
        "    ),\n",
        "    go.Scatter3d(\n",
        "        x=x, y=y, z=z, mode=\"markers\",\n",
        "        marker=dict(size=10, sizemode=\"diameter\", color=\"rgba(0,0,0,0)\", opacity=0.0),\n",
        "        hovertext=hover_text, hoverinfo=\"text\",\n",
        "        hoverlabel=dict(bgcolor=\"black\", font_size=12, font_color=\"#39FF14\", bordercolor=\"#39FF14\", namelength=0),\n",
        "        showlegend=False\n",
        "    )\n",
        "])\n",
        "\n",
        "fig.update_layout(\n",
        "    scene=dict(\n",
        "        xaxis=dict(visible=False), yaxis=dict(visible=False), zaxis=dict(visible=False),\n",
        "        bgcolor=\"black\", aspectmode=\"data\",\n",
        "        camera=dict(eye=dict(x=1.8, y=1.8, z=1.2))\n",
        "    ),\n",
        "    paper_bgcolor=\"black\", plot_bgcolor=\"black\",\n",
        "    margin=dict(l=0, r=0, t=50, b=0),\n",
        "    title=\"üåê Weather Stations Globe\",\n",
        "    font=dict(size=10, color=\"white\"),\n",
        "    hovermode=\"closest\", hoverdistance=2\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GvHnhuB9IDhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code builds a fully interactive 3D weather globe: latitude/longitude are mapped onto a sphere, stations are plotted as temperature-colored points, and hovering reveals detailed climate stats like dew point, wind speed, and pressure. It‚Äôs essentially turning raw data into a visual planetary dashboard.\n",
        "\n",
        "**Navigation tips (laptops):** Drag with two fingers horizontally to rotate üåç, Drag with two fingers vertically to zoom in/out üîé, double-click hold and drag to span and click the home icon in the top right corner to get back in the original position üîÑ.\n"
      ],
      "metadata": {
        "id": "YeoX0HnPJX0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import joblib\n",
        "import pandas as pd\n",
        "from io import BytesIO\n",
        "\n",
        "# 1Ô∏è‚É£ Fetching the model from GitHub directly into memory\n",
        "url = \"https://github.com/praxavv/sustainability-tracker/raw/main/temp_c_boosted.pkl\"\n",
        "response = requests.get(url)\n",
        "response.raise_for_status()\n",
        "\n",
        "# 2Ô∏è‚É£ Loading the model from bytes without saving to disk\n",
        "model = joblib.load(BytesIO(response.content))\n",
        "\n",
        "# 3Ô∏è‚É£ Example prediction\n",
        "example_input = pd.DataFrame([{\n",
        "    'lat': 19.0785, 'lon': 72.8782, 'month': 9, 'day': 3,\n",
        "    'dewp_c': 25, 'wind_speed_m_s': 2, 'visibility_km': 10,\n",
        "    'precipitation_mm': 0, 'pressure_hpa': 1012\n",
        "}])\n",
        "\n",
        "pred = model.predict(example_input)\n",
        "print(\"Example prediction:\", round(pred[0], 2))"
      ],
      "metadata": {
        "id": "P5cCFz5zgX3i",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We queried historical weather data from BigQuery, selected relevant features, and trained an XGBoost regressor to predict daily temperature. The model was saved as temp_c_boosted.pkl for inference and demonstration."
      ],
      "metadata": {
        "id": "xfK4MOatNqZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import gdown\n",
        "import requests\n",
        "import json\n",
        "\n",
        "# --------------------- Config ---------------------\n",
        "gdrive_file_id = \"1snwv4uqwb-A-cFr54oqUwqwPPIAt-MT9\"\n",
        "gdrive_filename = \"Test-Predictions.csv\"\n",
        "github_meta_url = \"https://github.com/praxavv/sustainability-tracker/raw/refs/heads/main/Data/cross-validation-rolling-window.json\"\n",
        "\n",
        "# --------------------- Fetch GitHub Metadata ---------------------\n",
        "response = requests.get(github_meta_url)\n",
        "meta_json = response.json()\n",
        "\n",
        "# Per-window performance\n",
        "df_window_perf = pd.DataFrame(meta_json[\"per_window_performance\"])\n",
        "print(\"Per-window performance:\")\n",
        "print(df_window_perf.to_string(index=False))\n",
        "\n",
        "# Overall validation performance\n",
        "print(\"\\nüîÆ Overall validation performance:\")\n",
        "df_overall_perf = pd.Series(meta_json[\"overall_validation_performance\"])\n",
        "print(df_overall_perf)\n",
        "\n",
        "# --------------------- Fetching Google Drive CSV ---------------------\n",
        "gdown.download(f\"https://drive.google.com/uc?id={gdrive_file_id}\", gdrive_filename, quiet=True)\n",
        "df_drive = pd.read_csv(gdrive_filename)\n",
        "\n",
        "# --------------------- Predictions ---------------------\n",
        "print(\"\\n==== Predictions (latest per country, max 5 rows) ====\")\n",
        "df_drive['date'] = pd.to_datetime(df_drive['date'])\n",
        "cols = ['station_name', 'country_code', 'date', 'prediction', 'actual_temp']\n",
        "df_sorted = df_drive[cols].sort_values(['country_code', 'date'], ascending=[True, False])\n",
        "\n",
        "desired_countries = ['US', 'CN', 'IN', 'RU', 'JP']\n",
        "selected = []\n",
        "seen = set()\n",
        "\n",
        "for cc in desired_countries:\n",
        "    subset = df_sorted[df_sorted['country_code'] == cc]\n",
        "    if subset.empty:\n",
        "        continue\n",
        "    latest_date = subset['date'].max()\n",
        "    row = subset[subset['date'] == latest_date].head(1)\n",
        "    selected.append(row)\n",
        "    seen.add(cc)\n",
        "\n",
        "if len(selected) < 5:\n",
        "    remaining = df_sorted[~df_sorted['country_code'].isin(seen)]\n",
        "    remaining_latest = (\n",
        "        remaining.sort_values('date', ascending=False)\n",
        "        .drop_duplicates('country_code')\n",
        "        .head(5 - len(selected))\n",
        "    )\n",
        "    selected.append(remaining_latest)\n",
        "\n",
        "# Final output as DataFrame\n",
        "if selected:\n",
        "    df_custom_head = pd.concat(selected, ignore_index=True)\n",
        "    df_custom_head['date'] = pd.to_datetime(df_custom_head['date']).dt.strftime('%Y-%m-%d')\n",
        "else:\n",
        "    df_custom_head = pd.DataFrame(columns=cols)  # empty fallback\n",
        "\n",
        "display(df_custom_head)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "w7cbzNvi7mdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model went through five different validation ‚Äúwindows‚Äù (think of them like time-slices of unseen data), and it handled them with impressive consistency. RMSE stays in the low-to-mid 2‚Äôs, which means the model‚Äôs temperature predictions usually miss the actual value by just a couple degrees Celsius. The MAE hovering around ~1.8 shows that most errors are even smaller on average.\n",
        "\n",
        "The crown jewel is the R¬≤ score: all windows stay well above 0.87, peaking at 0.97 in val_1, which tells us the model explains over 90% of the variance in the data. The explained variance metric mirrors that, showing stability across slices.\n",
        "\n",
        "üîÆ Big picture: With an overall RMSE ‚âà 2.5 and R¬≤ ‚âà 0.91, this boosted model isn‚Äôt just guessing ‚Äî it‚Äôs capturing real structure in the weather data while keeping errors impressively tight."
      ],
      "metadata": {
        "id": "uerF8kdVOwkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculating absolute error\n",
        "df_drive['abs_error'] = abs(df_drive['prediction'] - df_drive['actual_temp'])\n",
        "\n",
        "plt.hist(df_drive['abs_error'], bins=100, log=True)\n",
        "plt.xlabel('Absolute Error (¬∞C)')\n",
        "plt.ylabel('Count (log scale)')\n",
        "plt.title('Error Distribution (Sep‚ÄìDec 2024)')\n",
        "plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GZ0ahUmNPu9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart shows how often different error magnitudes occur, with most predictions staying within a small temperature miss."
      ],
      "metadata": {
        "id": "m0BM3x7lPA0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: !pip install ipywidgets for better sync\n",
        "\n",
        "import os, urllib.request, joblib, numpy as np, pandas as pd\n",
        "from geopy.geocoders import Nominatim\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, clear_output\n",
        "from datetime import datetime\n",
        "\n",
        "# --- config / downloads ---\n",
        "MODEL_URL = \"https://github.com/praxavv/sustainability-tracker/raw/main/temp_c_boosted.pkl\"\n",
        "MODEL_PATH = \"temp_c_boosted.pkl\"\n",
        "TRAIN_S3 = \"https://praxavv.s3.eu-north-1.amazonaws.com/gsod_train_features.parquet\"\n",
        "DAILY_AGG_S3 = \"https://praxavv.s3.eu-north-1.amazonaws.com/gsod_daily_aggregated.parquet\"\n",
        "\n",
        "if os.path.exists(MODEL_PATH): os.remove(MODEL_PATH)\n",
        "urllib.request.urlretrieve(MODEL_URL, MODEL_PATH)\n",
        "model = joblib.load(MODEL_PATH)\n",
        "\n",
        "expected = list(getattr(model, \"feature_names_in_\", []))\n",
        "geolocator = Nominatim(user_agent=\"geoapi\")\n",
        "\n",
        "train_df = pd.read_parquet(TRAIN_S3)\n",
        "daily_df = pd.read_parquet(DAILY_AGG_S3)\n",
        "\n",
        "# --- prediction logic ---\n",
        "def run_prediction(city, country, date):\n",
        "    try:\n",
        "        dt = datetime.strptime(date, \"%Y-%m-%d\")\n",
        "        month, day = dt.month, dt.day\n",
        "        out = [\"<h3>Weather Prediction Dashboard üå°Ô∏è</h3>\"]\n",
        "\n",
        "        loc = geolocator.geocode(f\"{city}, {country}\")\n",
        "        if not loc:\n",
        "            display(HTML(f\"<p style='color:red;'>‚ö†Ô∏è Could not geocode {city}, {country}</p>\")); return\n",
        "        lat, lon = round(loc.latitude, 2), round(loc.longitude, 2)\n",
        "        out.append(f\"<p>‚úÖ Coordinates for <b>{city.strip()}, {country.strip()}</b> ‚Üí lat={lat}, lon={lon}</p>\")\n",
        "\n",
        "        subset = train_df[(train_df['month']==month) & (train_df['day']==day)].copy()\n",
        "        if subset.empty:\n",
        "            display(HTML(\"<p style='color:red;'>‚ö†Ô∏è No training row found for this date.</p>\")); return\n",
        "        subset['dist'] = np.hypot(subset['lat'] - lat, subset['lon'] - lon)\n",
        "        nearest = subset.loc[subset['dist'].idxmin()].round(2).to_dict()\n",
        "        out.append(f\"<p>‚úÖ Features filled from nearest training row at lat={nearest['lat']}, lon={nearest['lon']}</p>\")\n",
        "\n",
        "        row = {\n",
        "            f: (nearest.get(f) if f in nearest else\n",
        "                (lat if f == \"lat\" else lon if f == \"lon\" else\n",
        "                 dt.year if f == \"year\" else month if f == \"month\" else\n",
        "                 day if f == \"day\" else np.nan))\n",
        "            for f in expected\n",
        "        }\n",
        "\n",
        "        X = pd.DataFrame([row], columns=expected)\n",
        "        pred = round(float(model.predict(X)[0]), 2)\n",
        "        out.append(f\"<p>‚úÖ Model Prediction = <b>{pred:.2f}¬∞C</b></p>\")\n",
        "\n",
        "        daily_df['dist'] = np.hypot(daily_df['lat'] - lat, daily_df['lon'] - lon)\n",
        "        nearest_station = daily_df.loc[daily_df['dist'].idxmin(), 'station_name']\n",
        "        out.append(f\"<p>‚úÖ Nearest Station ‚Üí <b>{nearest_station}</b></p>\")\n",
        "        out.append(f\"<h2>üå°Ô∏è Predicted Temp on {date}: {pred:.2f}¬∞C</h2>\")\n",
        "\n",
        "        output_widget.clear_output(wait=True)\n",
        "        with output_widget:\n",
        "            display(HTML(\"\".join(out)))\n",
        "\n",
        "    except Exception as e:\n",
        "        clear_output(wait=True)\n",
        "        display(HTML(f\"<p style='color:red;'>‚ö†Ô∏è Error: {e}</p>\"))\n",
        "\n",
        "# --- widget ---\n",
        "FIELD_WIDTH, DESC_WIDTH = \"300px\", \"110px\"\n",
        "city_input = widgets.Text(description=\"City:\", placeholder=\"e.g., New York\",\n",
        "                          layout=widgets.Layout(width=FIELD_WIDTH),\n",
        "                          style={\"description_width\": DESC_WIDTH})\n",
        "country_input = widgets.Text(description=\"Country Code:\", placeholder=\"(e.g. US, IN)\",\n",
        "                             layout=widgets.Layout(width=FIELD_WIDTH),\n",
        "                             style={\"description_width\": DESC_WIDTH})\n",
        "date_input = widgets.DatePicker(description=\"Date:\", layout=widgets.Layout(width=FIELD_WIDTH),\n",
        "                                style={\"description_width\": DESC_WIDTH})\n",
        "predict_button = widgets.Button(description=\"Predict üå°Ô∏è\", button_style=\"success\",\n",
        "                                layout=widgets.Layout(width=\"160px\"))\n",
        "output_widget = widgets.Output()\n",
        "\n",
        "def on_predict_clicked(b):\n",
        "    with output_widget:\n",
        "        if not date_input.value:\n",
        "            display(HTML(\"<p style='color:red;'>‚ö†Ô∏è Please select a date.</p>\")); return\n",
        "        run_prediction(city_input.value.strip(), country_input.value.strip(),\n",
        "                       date_input.value.strftime(\"%Y-%m-%d\"))\n",
        "\n",
        "predict_button.on_click(on_predict_clicked)\n",
        "\n",
        "inputs = widgets.VBox([city_input, country_input, date_input,\n",
        "                       widgets.HBox([widgets.Box(layout=widgets.Layout(flex=\"1\")), predict_button],\n",
        "                                    layout=widgets.Layout(width=\"304.5px\"))])\n",
        "dashboard = widgets.VBox([inputs, output_widget], layout=widgets.Layout(align_items=\"flex-start\"))\n",
        "display(dashboard)"
      ],
      "metadata": {
        "id": "ekRSUf9naPat",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Impact Statement**\n",
        "\n",
        "By converting cleaned NOAA GSOD data (2018‚Äì2024) into an interactive globe with integrated predictive models, Sustainability-Tracker empowers diverse stakeholders‚Äîfarmers, energy planners, and city officials‚Äîto visually explore forecasts, run scenario experiments, and make more informed operational decisions. When paired with the experiment widget, the platform facilitates rapid hypothesis testing and localised decision support, lowering the barrier between climate data and action.\n",
        "\n",
        ">**Conclusion**\n",
        "\n",
        "This sample notebook showcases how historical data, ML, and interactive visualization can work together to produce short-term temperature forecasts, with scalable BigQuery access, batch + interactive inference, and a reproducible workflow‚Äîwhile leaving room to improve coverage, uncertainty, and latency in future iterations."
      ],
      "metadata": {
        "id": "aUxa6GB4PDhW"
      }
    }
  ]
}